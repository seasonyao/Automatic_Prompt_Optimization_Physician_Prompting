{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8927,
     "status": "ok",
     "timestamp": 1693246244750,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "6LWo86145qpK"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1509,
     "status": "ok",
     "timestamp": 1693246246250,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "glxoNu-_nstK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1693246313567,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "U4UTgnQehqoR"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('gpt_best_prompts_summs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1693246314683,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "dnqr9T_wR793"
   },
   "outputs": [],
   "source": [
    "openai.api_key = #insert API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1693246315990,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "7-2tnuw5Rbw0"
   },
   "outputs": [],
   "source": [
    "def get_score(prompt):\n",
    "  response = openai.ChatCompletion.create(\n",
    "              model=\"gpt-4\",\n",
    "              messages=[\n",
    "                  {\"role\": \"system\", \"content\": \"\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt},\n",
    "              ],\n",
    "              temperature=0.1,\n",
    "              max_tokens=1,\n",
    "              timeout=45,\n",
    "              n=1,\n",
    "              stop=None\n",
    "          )\n",
    "\n",
    "  return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1693250852007,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "zEPfyfnzTTF9"
   },
   "outputs": [],
   "source": [
    "def get_prompt(gt_sum, model_sum):\n",
    "  prompt = \"\"\"\n",
    "Task: \\nIm giving you two medical summaries. The first summary is the ground-truth one. The second summary is an AI model generated one.\\\n",
    " First, see if the ground-truth summary has any medical terms. If it doesn't, output 'None'. If it does, then please compare the\\\n",
    " AI generated summary with the ground-truth summary to see if its medical terms are accurate to the ones in the ground-truth summary\\\n",
    " and if the medical facts are accurate and similar/same as the ones in the ground-truth summary, output a score from 1 to 5 where 1\\\n",
    " is the majority/all medical terms and facts are not the same and incorrect, 3 is about half, and 5 is the\\\n",
    " majority/all. \\n\\nGround-truth summary: \\n{ground_truth} \\n\\nAI model summary: \\n{model} \\n\\nScore:\n",
    "  \"\"\".format(ground_truth=gt_sum, model=model_sum)\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1693246372740,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "tDAyds2EU90i"
   },
   "outputs": [],
   "source": [
    "models = [\"text-ada-001\", \"text-babbage-001\",\"text-curie-001\", \"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1693246336790,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "NlhNC5XzDgOA"
   },
   "outputs": [],
   "source": [
    "# comment this cell if using df instead of sub_df\n",
    "sub_df = df.head(5).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20386,
     "status": "ok",
     "timestamp": 1693246400309,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "t7kqPGB_Bvi3"
   },
   "outputs": [],
   "source": [
    "# replace all \"sub_df\" with \"df\"\n",
    "new_columns = sub_df.apply(\n",
    "    lambda row: pd.Series(\n",
    "        {f'gpt-4_factuality_score_{mod}': get_score(get_prompt(row['section_text'], row[mod]))\n",
    "         for mod in models}\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "sub_df = pd.concat([sub_df, new_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693253346875,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "mvWPNJOL7gIf",
    "outputId": "c713dd28-5376-4086-b598-a779eba713af"
   },
   "outputs": [],
   "source": [
    "#can be changed to \"df\" to view the resulting df\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1693253374021,
     "user": {
      "displayName": "Ahmed Jaafar",
      "userId": "01925140053924734789"
     },
     "user_tz": 240
    },
    "id": "Ggqb58sUw6By"
   },
   "outputs": [],
   "source": [
    "#change from \"sub_df\" to \"df\"\n",
    "sub_df.to_csv(\"gpt4_factuality_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPeg+dyZCu5g9zAUIUCS1Zo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (4.8.3-jupyterhub-stable) *",
   "language": "python",
   "name": "conda-env-4.8.3-jupyterhub-stable-py"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
