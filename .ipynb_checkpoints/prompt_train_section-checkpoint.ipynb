{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477749b6-e58c-44e2-97ab-8f496f40207a",
   "metadata": {
    "id": "477749b6-e58c-44e2-97ab-8f496f40207a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# OPENAI_API_KEY = \n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "import openai\n",
    "# openai.organization =\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from metrics import Rouge, AutomaticNgramEval, AutomaticFactEval\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3051615-a791-4be1-8081-65e344ed72d4",
   "metadata": {
    "id": "c3051615-a791-4be1-8081-65e344ed72d4"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('presidio/train.csv')\n",
    "eval_df = pd.read_csv('presidio/dev.csv')\n",
    "test_df = pd.read_csv('presidio/test.csv')\n",
    "\n",
    "train_df.rename(columns={'text': 'Conv_snippet'}, inplace=True)\n",
    "eval_df.rename(columns={'text': 'Conv_snippet'}, inplace=True)\n",
    "test_df.rename(columns={'text': 'Conv_snippet'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44752f78-6731-446c-a261-494b224fbe72",
   "metadata": {
    "id": "44752f78-6731-446c-a261-494b224fbe72"
   },
   "outputs": [],
   "source": [
    "target_section = 'Assessment_and_Plan:Prescriptions_and_Therapeutics'\n",
    "# 'Objective:Laboratory_and_Imaging_Results',\n",
    "# 'Subjective:Review_of_Systems', 'Subjective:Miscellaneous',\n",
    "# 'Assessment_and_Plan:Assessment', 'Subjective:Chief_Complaint',\n",
    "# 'Subjective:Medications', 'Subjective:Past_Medical_History',\n",
    "# 'Assessment_and_Plan:Diagnostics_and_Appointments',\n",
    "# 'Other:Healthcare_Complaints',\n",
    "# 'Assessment_and_Plan:Prescriptions_and_Therapeutics',\n",
    "# 'Subjective:Past_Surgical_History', 'Objective:Immunizations',\n",
    "# 'Subjective:Family_Medical_History', 'Subjective:Social_History',\n",
    "# 'Subjective:Allergies'\n",
    "\n",
    "train_df = train_df[train_df['section']==target_section]\n",
    "train_df = train_df.iloc[:50]\n",
    "eval_df = eval_df[eval_df['section']==target_section]\n",
    "eval_df = eval_df.iloc[:50]\n",
    "test_df = test_df[test_df['section']==target_section]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176fdc1-b61d-4ed9-8fae-9b0e2ac806c1",
   "metadata": {
    "id": "1176fdc1-b61d-4ed9-8fae-9b0e2ac806c1"
   },
   "outputs": [],
   "source": [
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Evaluation set size: {len(eval_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bd3e1-99e0-4a36-9bab-151d9abb9601",
   "metadata": {
    "id": "8d9bd3e1-99e0-4a36-9bab-151d9abb9601"
   },
   "outputs": [],
   "source": [
    "logging = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a6ab4-eb12-4d0c-9df4-c9d81ac280c2",
   "metadata": {
    "id": "fc2a6ab4-eb12-4d0c-9df4-c9d81ac280c2"
   },
   "outputs": [],
   "source": [
    "def dataloader(train_df, bsz,\n",
    "               target_trainable_instruction, rating_raw_instruction,\n",
    "               target_trainable_few_shot_examples, do_few_shot,\n",
    "               ngram_eval, factev,\n",
    "               sample_mode='random'):\n",
    "    if sample_mode == 'random':\n",
    "        sampled_data = train_df.sample(n=bsz)\n",
    "\n",
    "    if sample_mode == 'hard_negative':\n",
    "        print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"eval results on all TRAIN DATA because of hard_negative sampling\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        eval_dict = eval_loop(train_df, target_trainable_instruction, rating_raw_instruction,\n",
    "                              target_trainable_few_shot_examples, do_few_shot,\n",
    "                              ngram_eval, factev, eval_training_step=True)\n",
    "\n",
    "        # Find indices with different values\n",
    "        different_indices = [i for i in range(len(eval_dict['labels'])) if eval_dict['labels'][i] != eval_dict['preds'][i]]\n",
    "        print('hard_negative target datapoints:', different_indices)\n",
    "\n",
    "        sampled_data = train_df.sample(n=bsz, weights=weights)\n",
    "\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc2039-1520-4399-a9c0-98373d95a089",
   "metadata": {
    "id": "82fc2039-1520-4399-a9c0-98373d95a089"
   },
   "outputs": [],
   "source": [
    "summarize_raw_instruction = \"\"\"[target_trainable_instruction]\n",
    "[target_trainable_few_shot_examples]\n",
    "\n",
    "SOAP note section:\n",
    "[section]\n",
    "Conversation snippet:\n",
    "[Conv_snippet]\n",
    "\n",
    "Output your summary.\n",
    "Return the output as a dictionary object, adhering to the following structure:\n",
    "{\"summary\": ...}\n",
    "Please provide your response solely in the dictionary format without including any additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d8367-fed6-41be-bcc8-17bfa936d051",
   "metadata": {
    "id": "fd4d8367-fed6-41be-bcc8-17bfa936d051"
   },
   "outputs": [],
   "source": [
    "def do_summarize(target_trainable_instruction, rating_raw_instruction, section, Conv_snippet,\n",
    "              target_trainable_few_shot_examples='', do_few_shot=False):\n",
    "    instruction = rating_raw_instruction.replace('[target_trainable_instruction]', target_trainable_instruction)\n",
    "    if do_few_shot:\n",
    "        instruction = instruction.replace('[target_trainable_few_shot_examples]', target_trainable_few_shot_examples)\n",
    "    else:\n",
    "        instruction = instruction.replace('[target_trainable_few_shot_examples]', '')\n",
    "    instruction = instruction.replace('[section]', section)\n",
    "    instruction = instruction.replace('[Conv_snippet]', Conv_snippet)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",#gpt-3.5-turbo-16k\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    summary = json.loads(response[\"choices\"][0][\"message\"][\"content\"], strict=False)['summary']\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400273a2-95e6-45a5-8622-367908e42b76",
   "metadata": {
    "id": "400273a2-95e6-45a5-8622-367908e42b76"
   },
   "outputs": [],
   "source": [
    "def eval_log(logging, epoch, eval_dict):\n",
    "    print('epoch:', epoch)\n",
    "\n",
    "    # Check if the epoch exists in the logging dictionary\n",
    "    if 'epoch'+str(epoch) not in logging:\n",
    "        logging['epoch'+str(epoch)]= {}  # Create a new dictionary for the epoch\n",
    "\n",
    "    for k, v in eval_dict.items():\n",
    "        logging['epoch'+str(epoch)][k] = v\n",
    "        if k != 'labels' and k != 'preds':\n",
    "            print('\\t', k, v)\n",
    "\n",
    "def eval_one_step(eval_data, target_trainable_instruction, summarize_raw_instruction, target_trainable_few_shot_examples='', do_few_shot=False):\n",
    "    section = eval_data['section']\n",
    "    Conv_snippet = eval_data['Conv_snippet']\n",
    "\n",
    "    # get the model's rating on training data before training\n",
    "    curr_summary = do_summarize(target_trainable_instruction,\n",
    "                                summarize_raw_instruction,\n",
    "                                section, Conv_snippet,\n",
    "                                target_trainable_few_shot_examples, do_few_shot)\n",
    "\n",
    "    return curr_summary\n",
    "\n",
    "def eval_loop(eval_df, target_trainable_instruction, summarize_raw_instruction, target_trainable_few_shot_examples, do_few_shot, ngram_eval, factev, eval_training_step=False):\n",
    "    summary_gpt = []\n",
    "    summary_doctor = []\n",
    "    for eval_step in tqdm(range(eval_df.shape[0]), desc=\"Evaluation\"):\n",
    "        eval_data = eval_df.iloc[eval_step]\n",
    "        try:\n",
    "            curr_summary = eval_one_step(eval_data, target_trainable_instruction, summarize_raw_instruction, target_trainable_few_shot_examples, do_few_shot)\n",
    "            summary_gpt.append(curr_summary)\n",
    "            summary_doctor.append(eval_df.iloc[eval_step]['summary'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # eval generated critique\n",
    "    eval_dict = ngram_eval.run_all_evaluation(summary_doctor, summary_gpt)\n",
    "    UMLS_dict = factev.run_source_concept_faithfulness(ref_sums = summary_doctor, gen_sums = summary_gpt)\n",
    "    del UMLS_dict['pred_concepts_term']\n",
    "    del UMLS_dict['pred_concepts_cuis']\n",
    "    eval_dict.update(UMLS_dict)\n",
    "\n",
    "    eval_dict = {'summary_'+k: round(v, 4) for k, v in eval_dict.items()}\n",
    "\n",
    "    eval_dict['labels'] = summary_doctor\n",
    "    eval_dict['preds'] = summary_gpt\n",
    "\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451697be-515c-4ae7-9126-902564b22ebb",
   "metadata": {
    "id": "451697be-515c-4ae7-9126-902564b22ebb"
   },
   "outputs": [],
   "source": [
    "target_trainable_instruction = \"\"\"In this task, we ask for your expertise in writing SOAP notes from the doctor-patient conversation.\n",
    "Mainly we provide the target section in the SOAP note and the conversation snippet.\n",
    "We need you to generate a summary for the respective snippet.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "training_prompt_forward = \"\"\"In this task, you need to provide suggestions to modify the instruction in our SOAP notes writing system, which uses a model to generate SOAP notes from the doctor-patient conversation according to manually created instructions.\n",
    "Specifically, we feed the AI a conversation snippet and the target section in the SOAP note and ask it to generate the corresponding summary.\n",
    "But we found that the instruction in the current system is not perfect, so we need you to modify the instruction for this model to improve our system.\n",
    "\n",
    "The instruction now in our rating system:\n",
    "[target_trainable_instruction]\n",
    "SOAP note section for summary:\n",
    "[section]\n",
    "Conversation snippet for the model:\n",
    "[Conv_snippet]\n",
    "Current AI summary:\n",
    "[AI_summary]\n",
    "Reference summary:\n",
    "[label_summary]\n",
    "\n",
    "Here are some of the requirements you need to be aware of when suggesting the instruction modification in our system:\n",
    "1) For better generalization, what you suggest should be abstracted as high-level criteria as much as possible instead of only describing the details\n",
    "2) We will improve the instructions based on your suggestions. If I re-provide the system with the conversation snippet and the target section in the SOAP note, it needs to be able to generate the reference summary using your new suggested instructions.\n",
    "3) The instruction now in our system is for the zero-shot setting, don't try to add any examples to the instruction.\n",
    "4) We are currently only focusing on this target section, so you don't need to consider the situation of other sections in the SOAP note, just optimize the instructions completely for this section.\n",
    "\n",
    "Let's think step by step. First, output your reasons for why the current instruction in the system cannot generate the correct reference summary, then output your suggestions to modify the instruction for our system.\n",
    "Return the output as a dictionary object, adhering to the following structure:\n",
    "{\"reasons\": ..., \"suggestions\": ...}\n",
    "Ensure the 'suggestions' only includes text but not a list. Please provide your response solely in the dictionary format without including any additional text.\n",
    "\"\"\"\n",
    "\n",
    "training_prompt_backward_prefix = \"\"\"In this task, you need to provide suggestions to modify the instruction in our SOAP notes writing system, which uses a model to generate SOAP notes from the doctor-patient conversation according to manually created instructions.\n",
    "Specifically, we feed the AI a conversation snippet and the target section in the SOAP note and ask it to generate the corresponding summary.\n",
    "But we found that the instruction in the current system is not perfect, so we need you to modify the instruction for this model to improve our system.\n",
    "\n",
    "The instruction now in our system:\n",
    "[target_trainable_instruction]\n",
    "\"\"\"\n",
    "\n",
    "training_prompt_backward_suggestions = \"\"\"Suggestions from summary [i]:\n",
    "[suggestions]\n",
    "\"\"\"\n",
    "\n",
    "training_prompt_backward_suffix = \"\"\"\n",
    "Here are some of the requirements you need to be aware of when modifying the instruction in our system:\n",
    "1) For better generalization, what you suggest should be abstracted as high-level criteria as much as possible instead of only describing the details\n",
    "2) We will improve the instructions based on your suggestions. If I re-provide the system with the conversation snippet and the target section in the SOAP note, it needs to be able to generate the reference summary using your new suggested instructions.\n",
    "3) The instruction now in our system is for the zero-shot setting, don't try to add any examples to the instruction.\n",
    "4) We are currently only focusing on this target section, so you don't need to consider the situation of other sections in the SOAP note, just optimize the instructions completely for this section.\n",
    "\n",
    "Let's think step by step. First, briefly summarize the suggestions of all the data to get a final suggestion containing only the highest priority requirement, then output your modified instruction for our system based on the final suggestion.\n",
    "Return the output as a dictionary object, adhering to the following structure:\n",
    "{\"final suggestion\": ..., \"new instruction\": ...}\n",
    "Please provide your response solely in the dictionary format without including any additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681b9b6-f381-43c1-9fe8-0e910f712a37",
   "metadata": {
    "id": "f681b9b6-f381-43c1-9fe8-0e910f712a37"
   },
   "outputs": [],
   "source": [
    "def training_forward_step(training_prompt_forward, target_trainable_instruction,\n",
    "                          section, Conv_snippet,\n",
    "                          AI_summary,\n",
    "                          label_summary,\n",
    "                          learning_temperature_rate=0):\n",
    "    instruction = training_prompt_forward.replace('[target_trainable_instruction]', target_trainable_instruction)\n",
    "    instruction = instruction.replace('[section]', section)\n",
    "    instruction = instruction.replace('[Conv_snippet]', Conv_snippet)\n",
    "    instruction = instruction.replace('[AI_summary]', AI_summary)\n",
    "    instruction = instruction.replace('[label_summary]', label_summary)\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\", #gpt-4\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction}\n",
    "        ],\n",
    "        temperature=learning_temperature_rate\n",
    "    )\n",
    "\n",
    "    suggestions = json.loads(response[\"choices\"][0][\"message\"][\"content\"], strict=False)['suggestions']\n",
    "\n",
    "    return suggestions\n",
    "\n",
    "def training_backward_step(training_prompt_backward_prefix,\n",
    "                           training_prompt_backward_suggestions,\n",
    "                           training_prompt_backward_suffix,\n",
    "                           target_trainable_instruction,\n",
    "                           bsz, bsz_suggestion,\n",
    "                           learning_temperature_rate=0):\n",
    "    # make backward instruction with prefix, suggestions, and suffix\n",
    "    instruction = training_prompt_backward_prefix.replace('[target_trainable_instruction]', target_trainable_instruction)\n",
    "    for i in range(bsz):\n",
    "        suggestions_instruction = training_prompt_backward_suggestions.replace('[i]', str(i+1))\n",
    "        suggestions_instruction = suggestions_instruction.replace('[suggestions]', bsz_suggestion[i]['suggestions'])\n",
    "        instruction = instruction + suggestions_instruction\n",
    "    instruction = instruction + training_prompt_backward_suffix\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\", #gpt-4\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction}\n",
    "        ],\n",
    "        temperature=learning_temperature_rate\n",
    "    )\n",
    "\n",
    "    response = json.loads(response[\"choices\"][0][\"message\"][\"content\"], strict=False)\n",
    "\n",
    "    new_target_trainable_instruction = response['new instruction']\n",
    "    final_suggestion = response['final suggestion']\n",
    "    print(\"final suggestion in this step: \", final_suggestion)\n",
    "\n",
    "    return new_target_trainable_instruction\n",
    "\n",
    "def train_one_step(epoch, step, training_data, bsz, target_trainable_instruction,\n",
    "                   summarize_raw_instruction,\n",
    "                   logging, learning_temperature_rate=0):\n",
    "\n",
    "    # get forward suggestions (cal loss)\n",
    "    bsz_suggestions = []\n",
    "    for i in tqdm(range(bsz), desc=\"batch cal loss\"):\n",
    "        section = training_data.iloc[i]['section']\n",
    "        Conv_snippet = training_data.iloc[i]['Conv_snippet']\n",
    "        label_summary = training_data.iloc[i]['summary']\n",
    "\n",
    "        # get the model's rating on training data before training\n",
    "        AI_summary = do_summarize(target_trainable_instruction,\n",
    "                                  summarize_raw_instruction,\n",
    "                                  section, Conv_snippet)\n",
    "\n",
    "        # get suggestions for every data in batch\n",
    "        suggestions = training_forward_step(training_prompt_forward, target_trainable_instruction, section,\n",
    "                                     Conv_snippet, AI_summary, label_summary,\n",
    "                                     learning_temperature_rate)\n",
    "\n",
    "        bsz_suggestions.append({'label_summary': label_summary,\n",
    "                                'AI_summary': AI_summary,\n",
    "                                'suggestions': suggestions})\n",
    "\n",
    "    # make backward update\n",
    "    new_target_trainable_instruction = training_backward_step(training_prompt_backward_prefix,\n",
    "                                                              training_prompt_backward_suggestions,\n",
    "                                                              training_prompt_backward_suffix,\n",
    "                                                              target_trainable_instruction,\n",
    "                                                              bsz,\n",
    "                                                              bsz_suggestions,\n",
    "                                                              learning_temperature_rate)\n",
    "    #update to get new target_trainable_instruction\n",
    "    target_trainable_instruction = new_target_trainable_instruction\n",
    "    is_updated = True\n",
    "\n",
    "    return target_trainable_instruction, is_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341e86a-674c-4af6-9ba4-65dd0756cac8",
   "metadata": {
    "id": "2341e86a-674c-4af6-9ba4-65dd0756cac8"
   },
   "outputs": [],
   "source": [
    "target_trainable_few_shot_examples = \"\"\"Here are some examples of different summaries in SOAP notes to serve as reference points:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "training_example_selection_forward = \"\"\"In this task, you need to make some examples for our SOAP notes writing system, which uses a model to generate SOAP notes from the doctor-patient conversation according to manually created instructions.\n",
    "Specifically, we feed the AI a conversation snippet and the target section in the SOAP note and ask it to generate the corresponding summary.\n",
    "But we found that the instruction in the current system is not perfect, so we need you to modify the instruction for this model to improve our system.\n",
    "I need you to first judge whether the given new data is suitable to be added to the examples of the rating system based on the existing instructions and examples.\n",
    "If your judgment is \"yes\", then I need you to simplify the content of this example. In the process of simplification, you need to discard the unimportant details in the conversation snippet and summary, and retain and emphasize the most typical content in this example to better help the subsequent prediction of other data.\n",
    "\n",
    "The instruction now in our rating system:\n",
    "[target_trainable_instruction]\n",
    "The examples now in our rating system:\n",
    "[target_trainable_few_shot_examples]\n",
    "\n",
    "Here is the new candidate.\n",
    "SOAP note section for summary:\n",
    "[section]\n",
    "Conversation snippet for the model:\n",
    "[Conv_snippet]\n",
    "Reference summary:\n",
    "[label_summary]\n",
    "\n",
    "Here are some of the requirements you need to be aware of when you try to add examples in our rating system:\n",
    "1) First consider whether the example is typical, and it can be used as the representatives to help predict other data in the future.\n",
    "2) If I re-provide you with the original conversation snippet and the target section in the SOAP note, you need to be able to generate the reference summary accurately based on this instruction and few-shot examples in our system.\n",
    "3) Only keep the most critical part in few-shot examples. You can use ellipses (...) to skip unimportant parts.\n",
    "4) We are currently only focusing on this target section, so you don't need to consider the situation of other sections in the SOAP note, just optimize the instructions completely for this section.\n",
    "5) Every example needs to include a SOAP note section, Conversation snippet, and ground truth summary.\n",
    "\n",
    "Let's think step by step. First, output your judgment on whether the new candidate is suitable to be added to the examples of the system.\n",
    "If your judgment is \"no\", output \"{}\" in \"new example\".\n",
    "If your judgment is \"yes\", output your simplified version example in \"new example\".\n",
    "Return the output as a dictionary object, adhering to the following structure:\n",
    "{\"judgment\": ..., \"new example\": ...}\n",
    "Ensure the 'judgment' only includes 'yes' or 'no'.\n",
    "Ensure the 'new example' is a dictionary with 'SOAP note section', 'Conversation snippet', 'Ground truth summary'.\n",
    "Please provide your response solely in the dictionary format without including any additional text.\n",
    "\"\"\"\n",
    "\n",
    "training_example_selection_backward_prefix = \"\"\"In this task, you need to make some examples for our SOAP notes writing system, which uses a model to generate SOAP notes from the doctor-patient conversation according to manually created instructions.\n",
    "Specifically, we feed the AI a conversation snippet and the target section in the SOAP note and ask it to generate the corresponding summary.\n",
    "But we found that the instruction in the current system is not perfect, so we need you to modify the instruction for this model to improve our system.\n",
    "I need you to first judge whether the given new data is suitable to be added to the examples of the rating system based on the existing instructions and examples.\n",
    "If your judgment is \"yes\", then I need you to simplify the content of this example. In the process of simplification, you need to discard the unimportant details in the conversation snippet and summary, and retain and emphasize the most typical content in this example to better help the subsequent prediction of other data.\n",
    "\n",
    "The instruction now in our rating system:\n",
    "[target_trainable_instruction]\n",
    "The examples now in our rating system:\n",
    "[target_trainable_few_shot_example]\n",
    "\"\"\"\n",
    "\n",
    "training_example_selection_backward_suggestions = \"\"\"Example [i]:\n",
    "[example]\n",
    "\"\"\"\n",
    "\n",
    "training_example_selection_backward_suffix = \"\"\"Here are some of the requirements you need to be aware of when you try to add examples in our system:\n",
    "1) First consider whether the example is typical, and it can be used as the representatives to help predict other data in the future.\n",
    "2) If I re-provide you with the original conversation snippet and the target section in the SOAP note, you need to be able to generate the reference summary accurately based on this instruction and few-shot examples in our system.\n",
    "3) Only keep the most critical part in few-shot examples. You can use ellipses (...) to skip unimportant parts.\n",
    "4) We are currently only focusing on this target section, so you don't need to consider the situation of other sections in the SOAP note, just optimize the instructions completely for this section.\n",
    "5) Every example needs to include a SOAP note section, Conversation snippet, and Ground truth summary.\n",
    "6) The format for \"new few shot examples\" should be:\n",
    "Example 1:\n",
    "SOAP note section:\n",
    "Conversation snippet:\n",
    "Ground truth summary:\n",
    "\n",
    "Modify the few shot examples in the current instructions, consider the original examples and the new examples together, and then give the new few shot examples in the instructions.\n",
    "Return the output as a dictionary object, adhering to the following structure:\n",
    "{\"new few shot examples\": ...}\n",
    "Ensure the 'new few shot examples' only includes text but not a list or dictionary.\n",
    "Please provide your response solely in the dictionary format without including any additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dee8c7-f765-431a-bbd0-05c609913c9f",
   "metadata": {
    "id": "46dee8c7-f765-431a-bbd0-05c609913c9f"
   },
   "outputs": [],
   "source": [
    "def training_example_selection_forward_step(training_example_selection_forward,\n",
    "                                            target_trainable_instruction,\n",
    "                                            target_trainable_few_shot_examples,\n",
    "                                            section, Conv_snippet,\n",
    "                                            label_summary,\n",
    "                                            learning_temperature_rate=0):\n",
    "    instruction = training_example_selection_forward.replace('[target_trainable_instruction]', target_trainable_instruction)\n",
    "    instruction = instruction.replace('[target_trainable_few_shot_examples]', target_trainable_few_shot_examples)\n",
    "    instruction = instruction.replace('[section]', section)\n",
    "    instruction = instruction.replace('[Conv_snippet]', Conv_snippet)\n",
    "    instruction = instruction.replace('[label_summary]', label_summary)\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\", #gpt-4\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction}\n",
    "        ],\n",
    "        temperature=learning_temperature_rate\n",
    "    )\n",
    "\n",
    "    judgment = json.loads(response[\"choices\"][0][\"message\"][\"content\"], strict=False)['judgment']\n",
    "    example = json.loads(response[\"choices\"][0][\"message\"][\"content\"], strict=False)['new example']\n",
    "\n",
    "    return {'judgment': judgment, 'example': example}\n",
    "\n",
    "def training_example_selection_backward_step(training_example_selection_backward_prefix,\n",
    "                           training_example_selection_backward_suggestions,\n",
    "                           training_example_selection_backward_suffix,\n",
    "                           target_trainable_instruction,\n",
    "                           target_trainable_few_shot_examples,\n",
    "                           bsz, new_examples,\n",
    "                           learning_temperature_rate=0):\n",
    "    # make backward instruction with prefix, suggestions, and suffix\n",
    "    instruction = training_example_selection_backward_prefix.replace('[target_trainable_instruction]', target_trainable_instruction)\n",
    "    instruction = instruction.replace('[target_trainable_few_shot_examples]', target_trainable_few_shot_examples)\n",
    "    for i in range(len(new_examples)):\n",
    "        new_example_instruction = training_example_selection_backward_suggestions.replace('[i]', str(i+1))\n",
    "        new_example_instruction = new_example_instruction.replace('[example]', new_examples[i]['example'])\n",
    "        instruction = instruction + new_example_instruction\n",
    "    instruction = instruction + training_example_selection_backward_suffix\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\", #gpt-4\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction}\n",
    "        ],\n",
    "        temperature=learning_temperature_rate\n",
    "    )\n",
    "\n",
    "    response = json.loads(response[\"choices\"][0][\"message\"][\"content\"], strict=False)\n",
    "\n",
    "    new_target_trainable_few_shot_examples = response['new few shot examples']\n",
    "\n",
    "    return new_target_trainable_few_shot_examples\n",
    "\n",
    "def train_one_step_for_few_shot_example(epoch, step, training_data, bsz,\n",
    "                                        target_trainable_instruction,\n",
    "                                        target_trainable_few_shot_examples,\n",
    "                                        summarize_raw_instruction,\n",
    "                                        logging, learning_temperature_rate=0):\n",
    "\n",
    "    # get forward new_examples\n",
    "    new_examples = []\n",
    "    for i in tqdm(range(bsz), desc=\"batch few_shot forward\"):\n",
    "        section = training_data.iloc[i]['section']\n",
    "        Conv_snippet = training_data.iloc[i]['Conv_snippet']\n",
    "        label_summary = training_data.iloc[i]['summary']\n",
    "\n",
    "        try:\n",
    "            # get suggestions for every data in batch\n",
    "            new_example = training_example_selection_forward_step(training_example_selection_forward,\n",
    "                                                target_trainable_instruction,\n",
    "                                                target_trainable_few_shot_examples,\n",
    "                                                section, Conv_snippet, label_summary,\n",
    "                                                learning_temperature_rate)\n",
    "\n",
    "\n",
    "            if new_example['judgment'] == 'yes':\n",
    "                example_content = '\\n'.join(['SOAP note section:', new_example['example']['SOAP note section'],\n",
    "                                             'Conversation snippet:', new_example['example']['Conversation snippet'],\n",
    "                                             'Ground truth summary:', new_example['example']['Ground truth summary']])\n",
    "                new_examples.append({'judgment': new_example['judgment'],\n",
    "                                     'example': example_content})\n",
    "        except:\n",
    "            print('Encounter some errors in training_example_selection_forward_step')\n",
    "            continue\n",
    "\n",
    "    # make backward update\n",
    "    new_target_trainable_few_shot_examples = training_example_selection_backward_step(training_example_selection_backward_prefix,\n",
    "                                                                                training_example_selection_backward_suggestions,\n",
    "                                                                                training_example_selection_backward_suffix,\n",
    "                                                                                target_trainable_instruction,\n",
    "                                                                                target_trainable_few_shot_examples,\n",
    "                                                                                bsz, new_examples,\n",
    "                                                                                learning_temperature_rate)\n",
    "\n",
    "    #update to get new target_trainable_instruction\n",
    "    target_trainable_few_shot_examples = new_target_trainable_few_shot_examples\n",
    "\n",
    "    return target_trainable_few_shot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4334563-0e23-441d-8d1c-20655d9e5302",
   "metadata": {
    "id": "d4334563-0e23-441d-8d1c-20655d9e5302"
   },
   "outputs": [],
   "source": [
    "def train_loop(train_df, eval_df, target_trainable_instruction, summarize_raw_instruction,\n",
    "               logging, ngram_eval, factev,\n",
    "               target_trainable_few_shot_examples='', do_few_shot=False,\n",
    "               EPOCH=1, steps_per_epoch=5, bsz=10,\n",
    "               eval_at_beginning=False, dataloader_sample_mode='random', learning_temperature_rate=0):\n",
    "\n",
    "    if eval_at_beginning:\n",
    "        print(\"the init target_trainable_instruction is:\")\n",
    "        print(target_trainable_instruction)\n",
    "        if do_few_shot:\n",
    "            print(\"the init target_trainable_few_shot_examples is:\")\n",
    "            print(target_trainable_few_shot_examples)\n",
    "\n",
    "        print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"eval results on all TRAINING DATA in the beginning\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "        eval_dict = eval_loop(train_df, target_trainable_instruction, summarize_raw_instruction,\n",
    "                              target_trainable_few_shot_examples, do_few_shot,\n",
    "                              ngram_eval, factev, eval_training_step=False)\n",
    "        eval_log(logging, -1, eval_dict)\n",
    "\n",
    "        print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"eval results on all EVAL DATA in the beginning\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "        eval_dict = eval_loop(eval_df, target_trainable_instruction, summarize_raw_instruction,\n",
    "                              target_trainable_few_shot_examples, do_few_shot,\n",
    "                              ngram_eval, factev, eval_training_step=False)\n",
    "        eval_log(logging, -1, eval_dict)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(str(epoch), \"EPOCH BEGIN--------------------------------------------------------------------------------------------------\")\n",
    "        print(str(epoch), \"EPOCH BEGIN--------------------------------------------------------------------------------------------------\")\n",
    "        print(str(epoch), \"EPOCH BEGIN--------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # Check if the epoch exists in the logging dictionary\n",
    "        if 'epoch'+str(epoch) not in logging:\n",
    "            logging['epoch'+str(epoch)] = {}  # Create a new dictionary for the epoch\n",
    "\n",
    "        any_change_in_this_epoch = False\n",
    "\n",
    "        for train_step in tqdm(range(steps_per_epoch), desc=\"Training\"):\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "            print(\"START NEW TRAINING STEP\")\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "\n",
    "            # try:\n",
    "            # load training data for the epoch\n",
    "            training_data = dataloader(train_df, bsz,\n",
    "                                       target_trainable_instruction, summarize_raw_instruction,\n",
    "                                       target_trainable_few_shot_examples, do_few_shot,\n",
    "                                       ngram_eval, factev,\n",
    "                                       dataloader_sample_mode)\n",
    "\n",
    "            #-----------\n",
    "            print(\"training metrics: before training step\")\n",
    "            eval_dict = eval_loop(training_data, target_trainable_instruction, summarize_raw_instruction,\n",
    "                                  target_trainable_few_shot_examples, do_few_shot,\n",
    "                                  ngram_eval, factev, eval_training_step=True)\n",
    "            eval_log(logging, epoch, eval_dict)\n",
    "            #-----------\n",
    "\n",
    "            print('training section:', training_data.index.tolist())\n",
    "\n",
    "            #train the \"instruction\"\n",
    "            if not do_few_shot:\n",
    "                target_trainable_instruction, is_updated = train_one_step(epoch, train_step, training_data, bsz,\n",
    "                                                                          target_trainable_instruction,\n",
    "                                                                          summarize_raw_instruction,\n",
    "                                                                          logging, learning_temperature_rate=learning_temperature_rate)\n",
    "\n",
    "            #train the few-shot-examples\n",
    "            if do_few_shot:\n",
    "                target_trainable_few_shot_examples = train_one_step_for_few_shot_example(epoch, train_step,\n",
    "                                                                                         training_data,\n",
    "                                                                                         bsz,\n",
    "                                                                                         target_trainable_instruction,\n",
    "                                                                                         target_trainable_few_shot_examples,\n",
    "                                                                                         summarize_raw_instruction,\n",
    "                                                                                         logging,\n",
    "                                                                                         learning_temperature_rate=0)\n",
    "                is_updated = True\n",
    "\n",
    "            if is_updated:\n",
    "                any_change_in_this_epoch = True\n",
    "\n",
    "            #-----------\n",
    "            print(\"training metrics: after training step\")\n",
    "            eval_dict = eval_loop(training_data, target_trainable_instruction, summarize_raw_instruction,\n",
    "                                  target_trainable_few_shot_examples, do_few_shot,\n",
    "                                  ngram_eval, factev, eval_training_step=True)\n",
    "            eval_log(logging, epoch, eval_dict)\n",
    "            #-----------\n",
    "            # except:\n",
    "            #     print('Encounter some errors from OpenAI API')\n",
    "            #     # print('Encounter some errors from OpenAI API, start to sleep 60s...')\n",
    "            #     # time.sleep(60)\n",
    "            #     # print('End sleep, resume the training')\n",
    "            #     continue\n",
    "\n",
    "\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "            print(\"END THIS TRAINING STEP\")\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "\n",
    "        print(str(epoch), \"EPOCH END----------------------------------------------------------------------------------------------------\")\n",
    "        print(str(epoch), \"EPOCH END----------------------------------------------------------------------------------------------------\")\n",
    "        print(str(epoch), \"EPOCH END----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # do eval\n",
    "        if any_change_in_this_epoch:\n",
    "            print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "            print(\"eval results on all TRAINING DATA for EPOCH\", str(epoch))\n",
    "            print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "            eval_dict = eval_loop(train_df, target_trainable_instruction, summarize_raw_instruction,\n",
    "                                  target_trainable_few_shot_examples, do_few_shot,\n",
    "                                  ngram_eval, factev, eval_training_step=False)\n",
    "            eval_log(logging, epoch, eval_dict)\n",
    "\n",
    "            print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "            print(\"eval results on all EVAL DATA for EPOCH\", str(epoch))\n",
    "            print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "            eval_dict = eval_loop(eval_df, target_trainable_instruction, summarize_raw_instruction,\n",
    "                                  target_trainable_few_shot_examples, do_few_shot,\n",
    "                                  ngram_eval, factev, eval_training_step=False)\n",
    "            eval_log(logging, epoch, eval_dict)\n",
    "\n",
    "            print(\"after curr epoch, the target_trainable_instruction is:\")\n",
    "            print(target_trainable_instruction)\n",
    "            if do_few_shot:\n",
    "                print(\"after curr epoch, the target_trainable_few_shot_examples is:\")\n",
    "                print(target_trainable_few_shot_examples)\n",
    "\n",
    "    return target_trainable_instruction, target_trainable_few_shot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d943fc2-66f9-454b-be6c-c3cc2096a7b8",
   "metadata": {
    "id": "1d943fc2-66f9-454b-be6c-c3cc2096a7b8"
   },
   "outputs": [],
   "source": [
    "ngram_eval = AutomaticNgramEval()\n",
    "factev = AutomaticFactEval()\n",
    "\n",
    "target_trainable_instruction, target_trainable_few_shot_examples = train_loop(train_df, eval_df,\n",
    "                                                                              target_trainable_instruction,\n",
    "                                                                              summarize_raw_instruction, logging,\n",
    "                                                                              ngram_eval, factev,\n",
    "                                                                              target_trainable_few_shot_examples,\n",
    "                                                                              do_few_shot=False,\n",
    "                                                                              EPOCH=50, steps_per_epoch=1, bsz=10,\n",
    "                                                                              eval_at_beginning=True,\n",
    "                                                                              dataloader_sample_mode='random',\n",
    "                                                                              learning_temperature_rate=0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (4.8.3-jupyterhub-stable) *",
   "language": "python",
   "name": "conda-env-4.8.3-jupyterhub-stable-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
